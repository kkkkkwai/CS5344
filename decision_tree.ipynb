{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Model and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "We construct a decition tree of depth 10 based on tokenized description and tags, and also other attributes like comment_disabled.\n",
    "\n",
    "First import modules and initialize spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/11/19 00:10:18 WARN Utils: Your hostname, KW resolves to a loopback address: 127.0.1.1; using 172.24.216.50 instead (on interface eth0)\n",
      "23/11/19 00:10:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/19 00:10:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from predict.decision_tree import *\n",
    "spark: SparkSession = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy-load the data into DataFrame. Pyspark DataFrame has similar API to pandas, and thus is more readable.\n",
    "\n",
    "Then Filter and cast the data to correct types. Tokenize the description and tag fields for model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[video_id: string, title: string, publishedAt: string, channelId: string, channelTitle: string, categoryId: string, trending_date: string, tags: string, view_count: string, likes: string, dislikes: string, comment_count: string, thumbnail_link: string, comments_disabled: string, ratings_disabled: string, description: string, category_name: string]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/kw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "file_list = [\n",
    "    f\"{ROOT_PATH}/{c}_youtube_trending_data_processed.csv\" for c in COUNTRIES\n",
    "]\n",
    "df = spark.read.option(\"header\", True).csv(file_list)\n",
    "print(df)\n",
    "\n",
    "df = filter_and_cast(df)\n",
    "df = tokenize_description(df)\n",
    "df = tokenize_tag(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the hashing function and the decision tree model to be fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42,[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41],[0.0018278573848525665,0.07464633900220487,0.12334376379985049,0.028529325879655464,0.031046733856399958,0.02460595751780358,0.008506165500510471,0.0151428801631184,0.016525395217440626,0.012582680834966888,0.020564151214719998,0.02549186190101141,0.013064011639899674,0.0051530214713479605,0.04920009146921148,0.003737898074440687,0.017573337674904443,0.018624275185159786,0.03895801335276766,0.016106560401864277,0.014763713640571502,0.0464948813974357,0.006854187144458909,0.004259017218670106,0.06680596816862805,0.028662138859354973,0.026826854802257362,0.016110971754540213,0.015344687830170508,0.013130711912632202,0.010088154180455182,0.04895480723521748,0.006158957042376181,0.027262245950435707,0.02274762327941597,0.010080178143945443,0.013879554288261486,0.01245376191055722,0.003289278176354053,0.023300910469035093,0.011321306283613653,0.025979768769482454])\n"
     ]
    }
   ],
   "source": [
    "hashing_text, hashing_tags, assembler = construct_hashing_assembler()\n",
    "df = hashing_text.transform(df)\n",
    "df = hashing_tags.transform(df)\n",
    "df = assembler.transform(df)\n",
    "\n",
    "regressor = DecisionTreeRegressor(\n",
    "    maxDepth=10,\n",
    "    maxMemoryInMB=512,\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"view_count\",\n",
    "    predictionCol=\"prediction\",\n",
    ")\n",
    "model = regressor.fit(df)\n",
    "print(model.featureImportances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different channels have vastly different number of subscribers. To better evaluate the model, we don't calculate the error on view count, but instead its rank inside the same channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/kw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/kw/miniconda3/envs/CS5344/lib/python3.9/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 11.356571662907525, MSE: 380.21283575968835\n"
     ]
    }
   ],
   "source": [
    "file_list = [\n",
    "    f\"{TEST_PATH}/{c}_youtube_trending_data_processed_test.csv\" for c in COUNTRIES\n",
    "]\n",
    "df = spark.read.option(\"header\", True).csv(file_list[0])\n",
    "\n",
    "df = filter_and_cast(df)\n",
    "df = tokenize_description(df)\n",
    "df = tokenize_tag(df)\n",
    "\n",
    "df = hashing_text.transform(df)\n",
    "df = hashing_tags.transform(df)\n",
    "df = assembler.transform(df)\n",
    "df = model.transform(df)\n",
    "\n",
    "# Eval based on rank instead of abs view count\n",
    "df = df.withColumn(\n",
    "    \"expected\",\n",
    "    dense_rank().over(Window.partitionBy(\"channelId\").orderBy(\"view_count\")),\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"predicted\",\n",
    "    dense_rank().over(Window.partitionBy(\"channelId\").orderBy(\"prediction\")),\n",
    ")\n",
    "df = df.withColumns(\n",
    "    {\n",
    "        \"expected\": df[\"expected\"].cast(\"double\"),\n",
    "        \"predicted\": df[\"predicted\"].cast(\"double\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "metrics = RegressionMetrics(df.select(\"expected\", \"predicted\").rdd.map(tuple))\n",
    "print(f\"MAE: {metrics.meanAbsoluteError}, MSE: {metrics.meanSquaredError}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sub_df = df.limit(100).select(\n",
    "    [\"video_id\", \"title\", \"channelId\", \"channelTitle\", \"expected\", \"predicted\"]\n",
    ")\n",
    "sub_df.write.csv(f\"{PREDICT_OUTPUT}/decision_tree\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS5344",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
