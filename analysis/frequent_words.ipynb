{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install dask_ml"
      ],
      "metadata": {
        "id": "K_pW-7WVQ2GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess"
      ],
      "metadata": {
        "id": "tfmOYyT4k5WL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYaPWAI45e1j"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import dask\n",
        "from dask.distributed import Client\n",
        "import dask.dataframe as dd\n",
        "import json\n",
        "import logging\n",
        "\n",
        "TRENDING_VIDEO_DATA = \"data/youtube-trending-video-dataset\"\n",
        "FOREIGN_LANGUAGES = [\"BR\", \"DE\",\"FR\", \"IN\", \"JP\", \"KR\", \"MX\", \"RU\"]\n",
        "\n",
        "def preprocess(data_path: Path, category_file: Path):\n",
        "    with category_file.open() as f:\n",
        "        cat_items = json.load(f)[\"items\"]\n",
        "    cat_dic = {int(c[\"id\"]): c[\"snippet\"][\"title\"] for c in cat_items}\n",
        "\n",
        "    ddf = dd.read_csv(data_path.as_posix())\n",
        "    ddf[\"category_name\"] = ddf[\"categoryId\"].map(cat_dic)\n",
        "    ddf[\"description\"] = ddf[\"description\"].fillna(\"\")\n",
        "\n",
        "    ddf.dask.visualize(filename=f\"{data_path.stem}_no_translation.svg\")\n",
        "    processed_file_name =  f\"{data_path.stem}_processed.csv\"\n",
        "    ddf.to_csv(data_path.parent.joinpath(processed_file_name).as_posix(), single_file=True, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "root_path = \"/content/drive/MyDrive/NUS/CS5344/\"\n",
        "for country in [\"CA\", \"GB\", \"US\"]:\n",
        "    data_path = Path(root_path + country + \"_youtube_trending_data.csv\")\n",
        "    category_file = Path(root_path + country + \"_category_id.json\")\n",
        "    preprocess(data_path=data_path, category_file=category_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfYMSzftA5gz",
        "outputId": "7fc23f17-7e0b-4f2f-d897-25e1aa896c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load your data into a Pandas DataFrame\n",
        "# Replace 'your_data.csv' with the actual filename or path to your data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/NUS/CS5344/CA_youtube_trending_data_processed.csv\", lineterminator='\\n')\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "zS5fpGu3F8jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "0VQoxJyZkWKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "if I have csv file which is the table contain column video_id, views, categoryId, catergory_name, title, trending_date, description. I want to find the top 10 frequent words in description. however, the video_id is not unique as the data may update when the date change. I want the data before 01, November 2023 for calculation. that means, if the video_id is repeated, used the latest date before 01, November 2023 in column \"trending_date\". please use pyspark and tfidf.\n"
      ],
      "metadata": {
        "id": "asPyALU-Gen_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "sPRhPDmI82yc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fbb44b8-fc5f-4148-a5c6-e1ff2dfb15c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample in pandas"
      ],
      "metadata": {
        "id": "Cr8Ug4f2kY0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "root_path = \"/content/drive/MyDrive/NUS/CS5344/\"\n",
        "for country in [\"CA\", \"GB\", \"US\"]:\n",
        "    data_path = root_path + country + \"_youtube_trending_data_processed.csv\"\n",
        "    vars()[country] = pd.read_csv(data_path, lineterminator='\\n')\n",
        "\n",
        "df = pd.concat([CA, GB, US], axis=0)\n",
        "\n",
        "# Handle missing values in the 'description' column\n",
        "df['description'] = df['description'].fillna('')\n",
        "\n",
        "# Filter data before 01, November 2023\n",
        "df['trending_date'] = pd.to_datetime(df['trending_date'])\n",
        "df = df[df['trending_date'] < \"2023-11-01\"]\n",
        "\n",
        "# Get the latest trending date for each video_id\n",
        "df['latest_trending_date'] = df.groupby('video_id')['trending_date'].transform('max')\n",
        "df = df[df['trending_date'] == df['latest_trending_date']]\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words and not any(stop_word in word.lower() for stop_word in [\"www\", \"http\", \"https\",'video', 'youtube', 'facebook', 'new', 'follow', 'like', 'watch', 'subscribe', 'channel'])]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Tokenize and remove stop words using scikit-learn TfidfVectorizer\n",
        "new = pd.concat([df['description'], df['title']], axis=0)\n",
        "vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize_and_remove_stopwords, max_features=10)\n",
        "X = vectorizer.fit_transform(new)\n",
        "\n",
        "# Sum the TF-IDF scores for each word across all documents\n",
        "sum_tfidf = X.sum(axis=0)\n",
        "\n",
        "# Get the indices of the top 10 words\n",
        "top_word_indices = sum_tfidf.argsort()[0, ::-1][:10]\n",
        "\n",
        "# Get the actual words using the indices\n",
        "top_words = [vectorizer.get_feature_names_out()[idx] for idx in top_word_indices]\n",
        "\n",
        "# Display the result\n",
        "print(\"Top 10 words based on TF-IDF:\")\n",
        "print(top_words)"
      ],
      "metadata": {
        "id": "Zy6sc4bQP8tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dask"
      ],
      "metadata": {
        "id": "vNdwpLfokb4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "from dask.distributed import Client\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Assuming you have a Dask cluster set up, you can create a client\n",
        "client = Client()\n",
        "\n",
        "root_path = \"/content/drive/MyDrive/NUS/CS5344/\"\n",
        "dfs = []\n",
        "\n",
        "for country in [\"CA\", \"GB\", \"US\"]:\n",
        "    data_path = root_path + country + \"_youtube_trending_data_processed.csv\"\n",
        "    df = dd.read_csv(data_path, lineterminator='\\n')\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenate without resetting the index\n",
        "df = dd.concat(dfs, axis=0)\n",
        "\n",
        "# Handle missing values in the 'description' column\n",
        "df['description'] = df['description'].fillna('')\n",
        "\n",
        "# Filter data before 01, November 2023\n",
        "df['trending_date'] = dd.to_datetime(df['trending_date'])\n",
        "df = df[df['trending_date'] < \"2023-11-01\"]\n",
        "\n",
        "# Get the latest trending date for each video_id\n",
        "df['latest_trending_date'] = df.groupby('video_id')['trending_date'].transform('max')\n",
        "df = df[df['trending_date'] == df['latest_trending_date']]\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words and not any(stop_word in word.lower() for stop_word in [\"www\", \"http\", \"https\", 'video', 'youtube', 'facebook', 'new', 'follow', 'like', 'watch', 'subscribe', 'channel'])]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Tokenize and remove stop words using scikit-learn TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize_and_remove_stopwords, max_features=10)\n",
        "\n",
        "# Concatenate description and title, tokenize and vectorize\n",
        "df['combined_text'] = df['description'] + ' ' + df['title']\n",
        "X = vectorizer.fit_transform(df['description'].compute())\n",
        "\n",
        "# Sum the TF-IDF scores for each word across all documents\n",
        "sum_tfidf = X.sum(axis=0)\n",
        "\n",
        "# Get the indices of the top 10 words\n",
        "top_word_indices = sum_tfidf.argsort()[0, ::-1][:10]\n",
        "\n",
        "# Get the actual words using the indices\n",
        "top_words = [vectorizer.get_feature_names_out()[idx] for idx in top_word_indices]\n",
        "\n",
        "# Display the result\n",
        "print(\"Top 10 words based on TF-IDF:\")\n",
        "print(top_words)\n"
      ],
      "metadata": {
        "id": "hsvy1SZ9kQpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pyspark"
      ],
      "metadata": {
        "id": "Orb0IGUIkeZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, max\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"YouTubeTrendingAnalysis\").getOrCreate()\n",
        "\n",
        "# Specify the root path\n",
        "root_path = \"/content/drive/MyDrive/NUS/CS5344/\"\n",
        "\n",
        "# Read data for each country into PySpark DataFrames\n",
        "countries = [\"CA\", \"GB\", \"US\"]\n",
        "dfs = []\n",
        "\n",
        "for country in countries:\n",
        "    data_path = root_path + country + \"_youtube_trending_data_processed.csv\"\n",
        "    df_country = spark.read.option(\"header\", \"true\").csv(data_path, multiLine=True)\n",
        "    dfs.append(df_country)\n",
        "\n",
        "# Concatenate the PySpark DataFrames\n",
        "df = reduce(lambda x, y: x.union(y), dfs)\n",
        "\n",
        "# Handle missing values in the 'description' column\n",
        "df = df.na.fill('')\n",
        "\n",
        "# Filter data before 01, November 2023\n",
        "df = df.withColumn(\"trending_date\", df[\"trending_date\"].cast(\"date\"))\n",
        "df = df.filter(col(\"trending_date\") < \"2023-11-01\")\n",
        "\n",
        "# Get the latest trending date for each video_id\n",
        "df = df.withColumn(\"latest_trending_date\", max(\"trending_date\").over(Window.partitionBy(\"video_id\")))\n",
        "df = df.filter(col(\"trending_date\") == col(\"latest_trending_date\")).drop(\"latest_trending_date\")\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"description_tokens\")\n",
        "df = tokenizer.transform(df)\n",
        "\n",
        "# Remove stopwords\n",
        "remover = StopWordsRemover(inputCol=\"description_tokens\", outputCol=\"filtered_tokens\")\n",
        "df = remover.transform(df)\n",
        "\n",
        "# Collect distinct words from the dataset\n",
        "distinct_words = df.selectExpr(\"explode(filtered_tokens) as word\").select(\"word\").distinct().collect()\n",
        "distinct_words = [row.word for row in distinct_words]\n",
        "\n",
        "# Tokenize and remove stop words using TF-IDF\n",
        "hashingTF = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"raw_features\", numFeatures=10)\n",
        "df = hashingTF.transform(df)\n",
        "\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "idf_model = idf.fit(df)\n",
        "df = idf_model.transform(df)\n",
        "\n",
        "# Create a mapping between index and word\n",
        "index_to_word = {idx: distinct_words[idx] for idx in range(len(distinct_words))}\n",
        "\n",
        "# Sum the TF-IDF scores for each word across all documents\n",
        "sum_tfidf = idf_model.idf.toArray()\n",
        "\n",
        "# Get the indices of the top 10 words\n",
        "top_word_indices = sorted(range(len(sum_tfidf)), key=lambda i: sum_tfidf[i], reverse=True)[:10]\n",
        "\n",
        "# Get the actual words using the mapping\n",
        "top_words = [index_to_word[idx] for idx in top_word_indices]\n",
        "\n",
        "# Display the result\n",
        "print(\"Top 10 words based on TF-IDF:\")\n",
        "print(top_words)\n"
      ],
      "metadata": {
        "id": "rfFX8EEVb6oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "1TRq6T8mklTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If I have csv file which is the table contain column video_id, views, categoryId, catergory_name, title, trending_date, view_count, description. The video_id is not unique as the data may update when the date change. I want the data before 01, November 2023 for calculation. that means, if the video_id is repeated, used the latest date before 01, November 2023 in column \"trending_date\".\n",
        "\n",
        "\n",
        "Group dataset a by view_count in quartile, find and compare top 30 frequent words in description\n",
        "\n",
        "please use pyspark and tfidf."
      ],
      "metadata": {
        "id": "kjLMH7fGs5yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample in pandas"
      ],
      "metadata": {
        "id": "87b6ij0LGnHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load your CSV file into a DataFrame\n",
        "root_path = \"/content/drive/MyDrive/NUS/CS5344/\"\n",
        "for country in [\"CA\", \"GB\", \"US\"]:\n",
        "    data_path = root_path + country + \"_youtube_trending_data_processed.csv\"\n",
        "    vars()[country] = pd.read_csv(data_path, lineterminator='\\n')\n",
        "\n",
        "df = pd.concat([CA, GB, US], axis=0)\n",
        "\n",
        "# Handle missing values in the 'description' column\n",
        "df['description'] = df['description'].fillna('')\n",
        "\n",
        "# Filter data before 01, November 2023\n",
        "df['trending_date'] = pd.to_datetime(df['trending_date'])\n",
        "df = df[df['trending_date'] < '2023-11-01']\n",
        "\n",
        "# Get the latest date for each video_id\n",
        "df = df.sort_values('trending_date', ascending=False).drop_duplicates('video_id')\n",
        "\n",
        "# Group by view_count quartiles\n",
        "df['view_count_quartile'] = pd.qcut(df['view_count'], q=4, labels=False)\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words and not any(stop_word in word.lower() for stop_word in [\"www\", \"http\", \"https\",'video', 'youtube', 'facebook', 'new', 'follow', 'like', 'watch', 'subscribe', 'channel'])]\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "df['filtered_words'] = df['description'].apply(tokenize_and_remove_stopwords)\n",
        "\n",
        "# Calculate TF-IDF for each quartile\n",
        "for quartile in range(4):\n",
        "    quartile_df = df[df['view_count_quartile'] == quartile]\n",
        "\n",
        "    corpus = quartile_df['description'].tolist()\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize_and_remove_stopwords)\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # Get feature names and sort by IDF values\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    idf_values = vectorizer.idf_\n",
        "    sorted_features = [feature for _, feature in sorted(zip(idf_values, feature_names))]\n",
        "\n",
        "    # Display the top 30 frequent words for each quartile\n",
        "    top_30_words = sorted_features[:10]\n",
        "    print(f\"Top 10 frequent words for Quartile {quartile + 1}: {top_30_words}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_RX7dae5waMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dask"
      ],
      "metadata": {
        "id": "eM1RuosXGqWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "from dask import delayed\n",
        "from dask.diagnostics import ProgressBar\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import csv  # Add this import statement\n",
        "import numpy as np\n",
        "\n",
        "# Load your CSV file into a Dask DataFrame\n",
        "root_path = \"/content/drive/MyDrive/NUS/CS5344/\"\n",
        "dfs = []\n",
        "for country in [\"CA\", \"GB\", \"US\"]:\n",
        "    data_path = root_path + country + \"_youtube_trending_data_processed.csv\"\n",
        "    df = dd.from_pandas(pd.read_csv(data_path, lineterminator='\\n'), npartitions=2)\n",
        "    dfs.append(df)\n",
        "\n",
        "ddf = dd.concat(dfs)\n",
        "\n",
        "# Handle missing values in the 'description' column\n",
        "ddf['description'] = ddf['description'].fillna('')\n",
        "\n",
        "# Filter data before 01, November 2023\n",
        "ddf['trending_date'] = dd.to_datetime(ddf['trending_date'])\n",
        "ddf = ddf[ddf['trending_date'] < '2023-11-01']\n",
        "\n",
        "# Get the latest date for each video_id\n",
        "ddf = ddf.sort_values('trending_date', ascending=False).drop_duplicates('video_id')\n",
        "\n",
        "# Define quartile boundaries\n",
        "quartile_boundaries = ddf['view_count'].quantile([0, 0.25, 0.5, 0.75, 1]).compute()\n",
        "\n",
        "# Create a new column 'view_count_quartile' based on quartiles\n",
        "ddf['view_count_quartile'] = dd.from_array(ddf['view_count'].map_partitions(np.digitize, bins=quartile_boundaries, right=True, meta=('x', 'i4')))\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "@delayed\n",
        "def tokenize_and_remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words and not any(stop_word in word.lower() for stop_word in [\"www\", \"http\", \"https\", 'video', 'youtube', 'facebook', 'new', 'follow', 'like', 'watch', 'subscribe', 'channel'])]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Apply the tokenize_and_remove_stopwords function using map\n",
        "ddf['filtered_words'] = ddf['description'].map(tokenize_and_remove_stopwords, meta=('x', 'object'))\n",
        "\n",
        "# Calculate TF-IDF for each quartile\n",
        "for quartile in range(4):\n",
        "    quartile_df = ddf[ddf['view_count_quartile'] == quartile]\n",
        "\n",
        "    corpus = quartile_df['description'].compute().tolist()\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', tokenizer=tokenize_and_remove_stopwords)\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    # Get feature names and sort by IDF values\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    idf_values = vectorizer.idf_\n",
        "    sorted_features = [feature for _, feature in sorted(zip(idf_values, feature_names))]\n",
        "\n",
        "    # Display the top 10 frequent words for each quartile\n",
        "    top_30_words = sorted_features[:10]\n",
        "    print(f\"Top 10 frequent words for Quartile {quartile + 1}: {top_30_words}\")\n"
      ],
      "metadata": {
        "id": "GWJ1jCb7Gs30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pyspark"
      ],
      "metadata": {
        "id": "ZsXJpg2rGtPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, row_number, udf\n",
        "from pyspark.sql.types import StringType, ArrayType\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, IDF\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import ntile\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"YouTubeTrendingAnalysis\").getOrCreate()\n",
        "\n",
        "# Load CSV files into PySpark DataFrames\n",
        "root_path = \"/content/drive/MyDrive/NUS/CS5344/\"\n",
        "countries = [\"CA\", \"GB\", \"US\"]\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for country in countries:\n",
        "    data_path = root_path + country + \"_youtube_trending_data_processed.csv\"\n",
        "    df_country = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "    dfs.append(df_country)\n",
        "\n",
        "# Concatenate PySpark DataFrames into a single DataFrame\n",
        "df = dfs[0]\n",
        "for df_country in dfs[1:]:\n",
        "    df = df.union(df_country)\n",
        "\n",
        "# Handle missing values in the 'description' column\n",
        "df = df.na.fill({'description': ''})\n",
        "\n",
        "# Convert the 'trending_date' column to timestamp\n",
        "df = df.withColumn('trending_date', col('trending_date').cast('timestamp'))\n",
        "\n",
        "# Filter data before 01, November 2023\n",
        "df = df.filter(col('trending_date') < '2023-11-01')\n",
        "\n",
        "# Get the latest date for each video_id\n",
        "window_spec = Window.partitionBy('video_id').orderBy(col('trending_date').desc())\n",
        "df = df.withColumn('rank', row_number().over(window_spec)).filter(col('rank') == 1).drop('rank')\n",
        "\n",
        "# Group by view_count quartiles\n",
        "df = df.withColumn('view_count_quartile', ntile(4).over(Window.orderBy('view_count')))\n",
        "\n",
        "# Tokenize and remove stopwords\n",
        "tokenizer = Tokenizer(inputCol='description', outputCol='words')\n",
        "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
        "\n",
        "# Convert array of strings to a single string\n",
        "concatenator = udf(lambda x: ' '.join(x), StringType())\n",
        "df = df.withColumn('filtered_words_concatenated', concatenator('filtered_words'))\n",
        "\n",
        "# Assemble features for TF-IDF\n",
        "assembler = VectorAssembler(inputCols=['filtered_words_concatenated'], outputCol='features')\n",
        "\n",
        "# Calculate TF-IDF for each quartile\n",
        "for quartile in range(4):\n",
        "    quartile_df = df.filter(col('view_count_quartile') == quartile)\n",
        "\n",
        "    # Create a pipeline for tokenization, stopwords removal, and TF-IDF\n",
        "    pipeline = Pipeline(stages=[tokenizer, remover, assembler, IDF(outputCol='tfidf_features')])\n",
        "    model = pipeline.fit(quartile_df)\n",
        "    quartile_df = model.transform(quartile_df)\n",
        "\n",
        "    # Display the top 10 frequent words for each quartile\n",
        "    top_10_words = quartile_df.select('tfidf_features').rdd.flatMap(lambda x: x['tfidf_features']).\\\n",
        "        map(lambda word: (word, top_words.count(word))).sortBy(lambda x: x[1], ascending=False).take(10)\n",
        "\n",
        "    print(f\"Top 10 frequent words for Quartile {quartile + 1}: {top_10_words}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1v_lgu_pwZEK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}