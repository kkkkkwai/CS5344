{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4af14b2-653f-47c7-95e7-e91738d53318",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16753b55-af9c-4ae9-95f4-05d500322f3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SETUP NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6fd6030-79e6-41f4-a2b8-4417d9ccdbae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.10/site-packages (3.7)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.10/site-packages (from nltk) (1.2.0)\nRequirement already satisfied: click in /databricks/python3/lib/python3.10/site-packages (from nltk) (8.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.10/site-packages (from nltk) (2022.7.9)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from nltk) (4.64.1)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988b08ee-a692-4761-9415-91103d0bdffb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### SETUP KAGGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cea1c36-e8a4-4ff2-be03-883e713203a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting kaggle\n  Downloading kaggle-1.5.16.tar.gz (83 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.6/83.6 kB 1.9 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nRequirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.16.0)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.10/site-packages (from kaggle) (2022.12.7)\nRequirement already satisfied: python-dateutil in /databricks/python3/lib/python3.10/site-packages (from kaggle) (2.8.2)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from kaggle) (2.28.1)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from kaggle) (4.64.1)\nCollecting python-slugify\n  Downloading python_slugify-8.0.1-py2.py3-none-any.whl (9.7 kB)\nRequirement already satisfied: urllib3 in /databricks/python3/lib/python3.10/site-packages (from kaggle) (1.26.14)\nRequirement already satisfied: bleach in /databricks/python3/lib/python3.10/site-packages (from kaggle) (4.1.0)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from bleach->kaggle) (22.0)\nRequirement already satisfied: webencodings in /databricks/python3/lib/python3.10/site-packages (from bleach->kaggle) (0.5.1)\nCollecting text-unidecode>=1.3\n  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.2/78.2 kB 4.4 MB/s eta 0:00:00\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->kaggle) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->kaggle) (2.0.4)\nBuilding wheels for collected packages: kaggle\n  Building wheel for kaggle (setup.py): started\n  Building wheel for kaggle (setup.py): finished with status 'done'\n  Created wheel for kaggle: filename=kaggle-1.5.16-py3-none-any.whl size=110686 sha256=6543836f4760bf95d9dd50a9cd2df67bb2e2abca13e982d80650a6a52a99c7dc\n  Stored in directory: /root/.cache/pip/wheels/b2/3d/88/839f363f3ce6b71785b8a95627cd52cb5359e54aba76a7ab76\nSuccessfully built kaggle\nInstalling collected packages: text-unidecode, python-slugify, kaggle\nSuccessfully installed kaggle-1.5.16 python-slugify-8.0.1 text-unidecode-1.3\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Install Kaggle\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5dc10b-eaa0-407f-b511-d8e566ca5d62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-20 06:06:19--  https://drive.google.com/uc?export=download&id=1lBmRzfsf4rui6bKc8LJUPP-pAr0wQAnK\r\nResolving drive.google.com (drive.google.com)... 142.250.69.206, 2607:f8b0:400a:80b::200e\r\nConnecting to drive.google.com (drive.google.com)|142.250.69.206|:443... connected.\r\nHTTP request sent, awaiting response... 303 See Other\r\nLocation: https://doc-00-bo-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/327cg19ufk38t5v0t8m4th4k1tsivnss/1700460375000/08408264012033939807/*/1lBmRzfsf4rui6bKc8LJUPP-pAr0wQAnK?e=download&uuid=dee965be-9929-41e8-80ff-3c32bd953807 [following]\r\nWarning: wildcards not supported in HTTP.\r\n--2023-11-20 06:06:19--  https://doc-00-bo-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/327cg19ufk38t5v0t8m4th4k1tsivnss/1700460375000/08408264012033939807/*/1lBmRzfsf4rui6bKc8LJUPP-pAr0wQAnK?e=download&uuid=dee965be-9929-41e8-80ff-3c32bd953807\r\nResolving doc-00-bo-docs.googleusercontent.com (doc-00-bo-docs.googleusercontent.com)... 142.250.217.97, 2607:f8b0:400a:806::2001\r\nConnecting to doc-00-bo-docs.googleusercontent.com (doc-00-bo-docs.googleusercontent.com)|142.250.217.97|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 66 [application/json]\r\nSaving to: ‘kaggle.json’\r\n\r\n\rkaggle.json           0%[                    ]       0  --.-KB/s               \rkaggle.json         100%[===================>]      66  --.-KB/s    in 0s      \r\n\r\n2023-11-20 06:06:19 (3.21 MB/s) - ‘kaggle.json’ saved [66/66]\r\n\r\n"
     ]
    }
   ],
   "source": [
    "# Get 'kaggle.json' for dataset downloading\n",
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1lBmRzfsf4rui6bKc8LJUPP-pAr0wQAnK' -O kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "668fb764-2410-4707-aeb2-4547ce211019",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Place 'kaggle.json' in Databricks file system\n",
    "dbutils.fs.cp(\"file:/databricks/driver/kaggle.json\", \"file:/root/.kaggle/kaggle.json\")\n",
    "# Ensure permissions are set correctly\n",
    "os.chmod(\"/root/.kaggle/kaggle.json\", 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5ec97d-8239-4cb3-bef0-c33e698c99af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21355b02-0cef-4da5-bf89-0d93a1c32ace",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### GET STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "170fa10f-d0ef-4fec-9a4d-4d3f0c8eee82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-20 06:06:26--  https://drive.google.com/uc?export=download&id=1o1C8DpOIcHahdZrtRm3iyRGg9zeTuSxv\r\nResolving drive.google.com (drive.google.com)... 142.250.69.206, 2607:f8b0:400a:80b::200e\r\nConnecting to drive.google.com (drive.google.com)|142.250.69.206|:443... connected.\r\nHTTP request sent, awaiting response... 303 See Other\r\nLocation: https://doc-0s-bo-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/r9b3ijfhb7a4sh594raou00c6c0q3shu/1700460375000/08408264012033939807/*/1o1C8DpOIcHahdZrtRm3iyRGg9zeTuSxv?e=download&uuid=6ef2795c-a383-4e7e-98fb-b405e197f0b3 [following]\r\nWarning: wildcards not supported in HTTP.\r\n--2023-11-20 06:06:26--  https://doc-0s-bo-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/r9b3ijfhb7a4sh594raou00c6c0q3shu/1700460375000/08408264012033939807/*/1o1C8DpOIcHahdZrtRm3iyRGg9zeTuSxv?e=download&uuid=6ef2795c-a383-4e7e-98fb-b405e197f0b3\r\nResolving doc-0s-bo-docs.googleusercontent.com (doc-0s-bo-docs.googleusercontent.com)... 142.250.217.97, 2607:f8b0:400a:806::2001\r\nConnecting to doc-0s-bo-docs.googleusercontent.com (doc-0s-bo-docs.googleusercontent.com)|142.250.217.97|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 4871 (4.8K) [text/plain]\r\nSaving to: ‘stopwords.txt’\r\n\r\n\rstopwords.txt         0%[                    ]       0  --.-KB/s               \rstopwords.txt       100%[===================>]   4.76K  --.-KB/s    in 0.001s  \r\n\r\n2023-11-20 06:06:30 (8.53 MB/s) - ‘stopwords.txt’ saved [4871/4871]\r\n\r\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1o1C8DpOIcHahdZrtRm3iyRGg9zeTuSxv' -O stopwords.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bba5730-9030-4948-aad4-dffd12d37683",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc615b48-aa78-4697-a7f6-ce2858022aed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading youtube-trending-video-dataset.zip to /databricks/driver\r\n\r  0%|                                               | 0.00/1.50G [00:00<?, ?B/s]\r  0%|                                      | 1.00M/1.50G [00:00<03:47, 7.10MB/s]\r  0%|                                      | 2.00M/1.50G [00:00<03:26, 7.79MB/s]\r  0%|                                      | 4.00M/1.50G [00:00<02:46, 9.66MB/s]\r  1%|▏                                     | 8.00M/1.50G [00:00<01:25, 18.7MB/s]\r  1%|▍                                     | 17.0M/1.50G [00:00<00:42, 37.8MB/s]\r  2%|▋                                     | 27.0M/1.50G [00:00<00:28, 55.3MB/s]\r  2%|▊                                     | 33.0M/1.50G [00:00<00:27, 57.1MB/s]\r  3%|█                                     | 42.0M/1.50G [00:01<00:23, 66.6MB/s]\r  3%|█▏                                    | 49.0M/1.50G [00:01<00:24, 64.5MB/s]\r  4%|█▍                                    | 57.0M/1.50G [00:01<00:23, 66.6MB/s]\r  4%|█▋                                    | 68.0M/1.50G [00:01<00:19, 78.6MB/s]\r  5%|█▉                                    | 78.0M/1.50G [00:01<00:18, 83.1MB/s]\r  6%|██▏                                   | 87.0M/1.50G [00:01<00:18, 80.6MB/s]\r  6%|██▎                                   | 95.0M/1.50G [00:01<00:21, 69.3MB/s]\r  7%|██▋                                    | 108M/1.50G [00:01<00:17, 85.7MB/s]\r  8%|███                                    | 120M/1.50G [00:01<00:15, 95.8MB/s]\r  9%|███▎                                   | 131M/1.50G [00:02<00:14, 99.1MB/s]\r  9%|███▊                                    | 145M/1.50G [00:02<00:13, 109MB/s]\r 10%|████                                    | 156M/1.50G [00:02<00:14, 101MB/s]\r 11%|████▍                                   | 169M/1.50G [00:02<00:13, 107MB/s]\r 12%|████▋                                   | 182M/1.50G [00:02<00:12, 113MB/s]\r 13%|█████                                   | 193M/1.50G [00:02<00:12, 109MB/s]\r 13%|█████▎                                  | 206M/1.50G [00:02<00:12, 113MB/s]\r 14%|█████▋                                  | 220M/1.50G [00:02<00:11, 121MB/s]\r 15%|██████                                  | 232M/1.50G [00:02<00:11, 122MB/s]\r 16%|██████▎                                 | 244M/1.50G [00:03<00:11, 121MB/s]\r 17%|██████▋                                 | 257M/1.50G [00:03<00:11, 121MB/s]\r 17%|██████▉                                 | 269M/1.50G [00:03<00:11, 120MB/s]\r 18%|███████▎                                | 283M/1.50G [00:03<00:10, 125MB/s]\r 19%|███████▋                                | 298M/1.50G [00:03<00:09, 133MB/s]\r 20%|████████                                | 312M/1.50G [00:03<00:09, 135MB/s]\r 21%|████████▍                               | 326M/1.50G [00:03<00:09, 134MB/s]\r 22%|████████▊                               | 340M/1.50G [00:03<00:09, 137MB/s]\r 23%|█████████▏                              | 354M/1.50G [00:03<00:09, 138MB/s]\r 24%|█████████▌                              | 368M/1.50G [00:04<00:09, 136MB/s]\r 25%|█████████▉                              | 381M/1.50G [00:04<00:09, 133MB/s]\r 26%|██████████▏                             | 394M/1.50G [00:04<00:09, 132MB/s]\r 27%|██████████▌                             | 408M/1.50G [00:04<00:08, 135MB/s]\r 27%|██████████▉                             | 421M/1.50G [00:04<00:08, 135MB/s]\r 28%|███████████▎                            | 435M/1.50G [00:04<00:08, 137MB/s]\r 29%|███████████▋                            | 449M/1.50G [00:04<00:08, 137MB/s]\r 30%|████████████                            | 463M/1.50G [00:04<00:08, 136MB/s]\r 31%|████████████▍                           | 477M/1.50G [00:04<00:08, 137MB/s]\r 32%|████████████▊                           | 491M/1.50G [00:04<00:08, 136MB/s]\r 33%|█████████████                           | 504M/1.50G [00:05<00:08, 127MB/s]\r 34%|█████████████▍                          | 518M/1.50G [00:05<00:08, 131MB/s]\r 34%|█████████████▊                          | 531M/1.50G [00:05<00:08, 128MB/s]\r 35%|██████████████▏                         | 544M/1.50G [00:05<00:08, 126MB/s]\r 36%|██████████████▌                         | 560M/1.50G [00:05<00:07, 138MB/s]\r 37%|██████████████▉                         | 574M/1.50G [00:05<00:07, 132MB/s]\r 38%|███████████████▎                        | 587M/1.50G [00:05<00:07, 133MB/s]\r 39%|███████████████▌                        | 600M/1.50G [00:05<00:07, 132MB/s]\r 40%|███████████████▉                        | 614M/1.50G [00:05<00:07, 135MB/s]\r 41%|████████████████▎                       | 627M/1.50G [00:06<00:07, 127MB/s]\r 42%|████████████████▋                       | 640M/1.50G [00:06<00:08, 107MB/s]\r 42%|████████████████▉                       | 654M/1.50G [00:06<00:08, 116MB/s]\r 43%|█████████████████▎                      | 666M/1.50G [00:06<00:07, 118MB/s]\r 44%|█████████████████▋                      | 679M/1.50G [00:06<00:07, 120MB/s]\r 45%|█████████████████▉                      | 691M/1.50G [00:06<00:07, 119MB/s]\r 46%|██████████████████▎                     | 704M/1.50G [00:06<00:07, 120MB/s]\r 47%|██████████████████▌                     | 716M/1.50G [00:06<00:07, 121MB/s]\r 47%|██████████████████▉                     | 730M/1.50G [00:07<00:06, 128MB/s]\r 48%|███████████████████▎                    | 743M/1.50G [00:07<00:06, 128MB/s]\r 49%|███████████████████▋                    | 756M/1.50G [00:07<00:06, 130MB/s]\r 50%|███████████████████▉                    | 769M/1.50G [00:07<00:06, 122MB/s]\r 51%|████████████████████▎                   | 784M/1.50G [00:07<00:06, 119MB/s]\r 52%|████████████████████▋                   | 796M/1.50G [00:07<00:06, 114MB/s]\r 53%|█████████████████████                   | 810M/1.50G [00:07<00:06, 122MB/s]\r 53%|█████████████████████▍                  | 823M/1.50G [00:07<00:05, 126MB/s]\r 54%|█████████████████████▊                  | 837M/1.50G [00:07<00:05, 130MB/s]\r 55%|██████████████████████                  | 850M/1.50G [00:08<00:06, 111MB/s]\r 56%|██████████████████████▍                 | 864M/1.50G [00:08<00:06, 118MB/s]\r 57%|██████████████████████▊                 | 876M/1.50G [00:08<00:06, 110MB/s]\r 58%|███████████████████████▏                | 894M/1.50G [00:08<00:05, 128MB/s]\r 59%|███████████████████████▌                | 907M/1.50G [00:08<00:05, 130MB/s]\r 60%|███████████████████████▉                | 920M/1.50G [00:08<00:05, 115MB/s]\r 61%|████████████████████████▏               | 933M/1.50G [00:08<00:05, 118MB/s]\r 61%|████████████████████████▌               | 945M/1.50G [00:08<00:05, 119MB/s]\r 62%|████████████████████████▉               | 959M/1.50G [00:08<00:04, 124MB/s]\r 63%|█████████████████████████▎              | 973M/1.50G [00:09<00:04, 129MB/s]\r 64%|█████████████████████████▌              | 986M/1.50G [00:09<00:04, 128MB/s]\r 65%|█████████████████████████▉              | 999M/1.50G [00:09<00:04, 126MB/s]\r 66%|█████████████████████████▋             | 0.99G/1.50G [00:09<00:04, 113MB/s]\r 67%|█████████████████████████▉             | 1.00G/1.50G [00:09<00:04, 118MB/s]\r 67%|██████████████████████████▎            | 1.01G/1.50G [00:09<00:04, 115MB/s]\r 68%|██████████████████████████▌            | 1.02G/1.50G [00:09<00:04, 109MB/s]\r 69%|██████████████████████████▉            | 1.04G/1.50G [00:09<00:04, 119MB/s]\r 70%|███████████████████████████▎           | 1.05G/1.50G [00:10<00:04, 118MB/s]\r 71%|███████████████████████████▋           | 1.07G/1.50G [00:10<00:03, 127MB/s]\r 72%|███████████████████████████▉           | 1.08G/1.50G [00:10<00:03, 117MB/s]\r 73%|████████████████████████████▎          | 1.09G/1.50G [00:10<00:03, 124MB/s]\r 73%|████████████████████████████▋          | 1.10G/1.50G [00:10<00:03, 127MB/s]\r 74%|████████████████████████████▉          | 1.12G/1.50G [00:10<00:03, 128MB/s]\r 75%|█████████████████████████████▎         | 1.13G/1.50G [00:10<00:03, 114MB/s]\r 76%|█████████████████████████████▌         | 1.14G/1.50G [00:10<00:03, 109MB/s]\r 77%|█████████████████████████████▉         | 1.15G/1.50G [00:10<00:03, 112MB/s]\r 78%|██████████████████████████████▎        | 1.17G/1.50G [00:11<00:02, 121MB/s]\r 78%|██████████████████████████████▌        | 1.18G/1.50G [00:11<00:02, 123MB/s]\r 79%|██████████████████████████████▉        | 1.19G/1.50G [00:11<00:03, 102MB/s]\r 80%|███████████████████████████████▏       | 1.20G/1.50G [00:11<00:03, 102MB/s]\r 81%|███████████████████████████████▋       | 1.22G/1.50G [00:11<00:02, 117MB/s]\r 82%|███████████████████████████████▉       | 1.23G/1.50G [00:11<00:02, 117MB/s]\r 83%|████████████████████████████████▎      | 1.24G/1.50G [00:11<00:02, 112MB/s]\r 83%|███████████████████████████████▋      | 1.25G/1.50G [00:11<00:02, 98.3MB/s]\r 84%|███████████████████████████████▉      | 1.26G/1.50G [00:12<00:03, 77.8MB/s]\r 85%|████████████████████████████████▏     | 1.27G/1.50G [00:12<00:03, 69.0MB/s]\r 85%|████████████████████████████████▎     | 1.28G/1.50G [00:12<00:03, 62.7MB/s]\r 86%|████████████████████████████████▌     | 1.29G/1.50G [00:12<00:03, 61.0MB/s]\r 86%|████████████████████████████████▋     | 1.29G/1.50G [00:12<00:03, 60.5MB/s]\r 87%|████████████████████████████████▉     | 1.30G/1.50G [00:12<00:03, 63.4MB/s]\r 87%|█████████████████████████████████     | 1.31G/1.50G [00:13<00:03, 62.9MB/s]\r 88%|█████████████████████████████████▎    | 1.32G/1.50G [00:13<00:03, 62.1MB/s]\r 88%|█████████████████████████████████▌    | 1.33G/1.50G [00:13<00:02, 74.1MB/s]\r 89%|█████████████████████████████████▋    | 1.33G/1.50G [00:13<00:02, 75.4MB/s]\r 89%|█████████████████████████████████▉    | 1.34G/1.50G [00:13<00:02, 76.8MB/s]\r 90%|██████████████████████████████████▏   | 1.35G/1.50G [00:13<00:02, 79.4MB/s]\r 90%|██████████████████████████████████▎   | 1.36G/1.50G [00:13<00:01, 79.0MB/s]\r 91%|██████████████████████████████████▌   | 1.37G/1.50G [00:13<00:01, 79.3MB/s]\r 91%|██████████████████████████████████▊   | 1.38G/1.50G [00:13<00:01, 77.9MB/s]\r 92%|██████████████████████████████████▉   | 1.38G/1.50G [00:14<00:01, 73.3MB/s]\r 93%|███████████████████████████████████▏  | 1.39G/1.50G [00:14<00:01, 75.5MB/s]\r 93%|███████████████████████████████████▎  | 1.40G/1.50G [00:14<00:01, 75.6MB/s]\r 94%|███████████████████████████████████▌  | 1.41G/1.50G [00:14<00:01, 67.0MB/s]\r 94%|███████████████████████████████████▋  | 1.41G/1.50G [00:14<00:01, 62.4MB/s]\r 94%|███████████████████████████████████▉  | 1.42G/1.50G [00:14<00:01, 63.8MB/s]\r 95%|████████████████████████████████████  | 1.43G/1.50G [00:14<00:01, 58.9MB/s]\r 95%|████████████████████████████████████▏ | 1.43G/1.50G [00:14<00:01, 61.9MB/s]\r 96%|████████████████████████████████████▍ | 1.44G/1.50G [00:15<00:01, 58.6MB/s]\r 96%|████████████████████████████████████▌ | 1.45G/1.50G [00:15<00:01, 53.5MB/s]\r 97%|████████████████████████████████████▋ | 1.45G/1.50G [00:15<00:01, 52.7MB/s]\r 97%|████████████████████████████████████▊ | 1.46G/1.50G [00:15<00:00, 53.1MB/s]\r 97%|█████████████████████████████████████ | 1.46G/1.50G [00:15<00:00, 55.5MB/s]\r 98%|█████████████████████████████████████▏| 1.47G/1.50G [00:15<00:00, 53.5MB/s]\r 98%|█████████████████████████████████████▎| 1.48G/1.50G [00:15<00:00, 53.3MB/s]\r 99%|█████████████████████████████████████▍| 1.48G/1.50G [00:15<00:00, 55.7MB/s]\r 99%|█████████████████████████████████████▌| 1.49G/1.50G [00:16<00:00, 55.3MB/s]\r 99%|█████████████████████████████████████▊| 1.49G/1.50G [00:16<00:00, 58.9MB/s]\r100%|█████████████████████████████████████▉| 1.50G/1.50G [00:16<00:00, 56.9MB/s]\r\n\r100%|██████████████████████████████████████| 1.50G/1.50G [00:16<00:00, 98.9MB/s]\r\nArchive:  youtube-trending-video-dataset.zip\r\n  inflating: BR_category_id.json     \r\n  inflating: BR_youtube_trending_data.csv  \r\n  inflating: CA_category_id.json     \r\n  inflating: CA_youtube_trending_data.csv  \r\n  inflating: DE_category_id.json     \r\n  inflating: DE_youtube_trending_data.csv  \r\n  inflating: FR_category_id.json     \r\n  inflating: FR_youtube_trending_data.csv  \r\n  inflating: GB_category_id.json     \r\n  inflating: GB_youtube_trending_data.csv  \r\n  inflating: IN_category_id.json     \r\n  inflating: IN_youtube_trending_data.csv  \r\n  inflating: JP_category_id.json     \r\n  inflating: JP_youtube_trending_data.csv  \r\n  inflating: KR_category_id.json     \r\n  inflating: KR_youtube_trending_data.csv  \r\n  inflating: MX_category_id.json     \r\n  inflating: MX_youtube_trending_data.csv  \r\n  inflating: RU_category_id.json     \r\n  inflating: RU_youtube_trending_data.csv  \r\n  inflating: US_category_id.json     \r\n  inflating: US_youtube_trending_data.csv  \r\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d rsrishav/youtube-trending-video-dataset\n",
    "!unzip youtube-trending-video-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0a583f-21c8-46fd-89f3-75642e54f294",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading top1000youtubers.zip to /databricks/driver\r\n\r  0%|                                               | 0.00/42.9k [00:00<?, ?B/s]\r\n\r100%|██████████████████████████████████████| 42.9k/42.9k [00:00<00:00, 5.47MB/s]\r\nArchive:  top1000youtubers.zip\r\n  inflating: youtubers_df.csv        \r\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d computingvictor/top1000youtubers\n",
    "!unzip top1000youtubers.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e2380bd-8944-44fc-b0aa-64b6b1460976",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3893914d-82ff-4daf-bff4-ac295466b431",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63a5102-2d30-49a0-96e1-5661232427e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e90867e2-f06a-4194-a215-353098e5de88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, udf, lit, lower, regexp_replace, min, max, first, unix_timestamp\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, BooleanType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c86584-3d6e-48ea-b996-a856c4fd8b47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Initialize Spark Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cada4e9-750e-4450-bd37-5b8d7011c39d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark Section\n",
    "spark = SparkSession.builder.appName(\"YoutubeTrendingVideoDataProcess\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a878f935-6225-4e17-8065-59c78c8ba05f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26850955-9a64-4be5-9a1c-2ff1e06b2001",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "class Tokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        with open(\"file:/databricks/driver/stopwords.txt\") as f:\n",
    "            custom_stopwords = [line.strip() for line in f]\n",
    "        self.all_stopwords = set(stopwords.words(\"english\")).union(set(custom_stopwords))\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        if isinstance(text, str):\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            return [x for x in tokens if x not in self.all_stopwords and len(x) > 1]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def tokenize_tag(self, tag_str):\n",
    "        if len(tag_str) > 0 and tag_str != \"[None]\":\n",
    "            return [x.strip() for x in tag_str.split(\"|\")]\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7054eb5d-a211-4b01-86fc-72ac12c173f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Category Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd241946-dbac-408e-9df2-c63ce350c125",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def categoryLoader(data):\n",
    "    data_list = []\n",
    "    for item in data[\"items\"]:\n",
    "        data_dict = {\n",
    "          \"id\": item['id'],\n",
    "          \"category\": item['snippet']['title'],\n",
    "        }\n",
    "        data_list.append(data_dict)\n",
    "    df = pd.DataFrame(data_list)\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef463171-324a-43d7-92c2-a17209045f4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Video Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f65ed35-bee7-4d2f-9cc8-84acd22aa79b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USEFUL_COLS = [\n",
    "    'video_id',\n",
    "    'title',\n",
    "    'publishedAt',\n",
    "    'channelId',\n",
    "    'channelTitle',\n",
    "    'categoryId',\n",
    "    'trending_date',\n",
    "    'tags',\n",
    "    'view_count',\n",
    "    'likes',\n",
    "    'dislikes',\n",
    "    'comment_count',\n",
    "    'description'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acee485c-01c7-4c3f-b7d3-2dda5aa6650e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# CSV File Loadfer\n",
    "def videoDataLoader(file_path, cols=USEFUL_COLS, token=True):\n",
    "    video_df = spark.read.csv(file_path, header=True)\n",
    "    df = video_df.select(*cols)\n",
    "\n",
    "    if token:\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenize_text_udf = udf(tokenizer.tokenize_text, ArrayType(StringType()))\n",
    "        tokenize_tag_udf = udf(tokenizer.tokenize_tag, ArrayType(StringType()))\n",
    "\n",
    "        df = df \\\n",
    "        .withColumn(\"title\", tokenize_text_udf(\"title\")) \\\n",
    "        .withColumn(\"channelTitle\",tokenize_text_udf(\"channelTitle\")) \\\n",
    "        .withColumn(\"tags\", tokenize_tag_udf(\"tags\")) \\\n",
    "        .withColumn(\"description\", tokenize_text_udf(\"description\"))\n",
    "\n",
    "    df = df.withColumn(\"publishedAt\", col(\"publishedAt\").cast(\"timestamp\")) \\\n",
    "           .withColumn(\"trending_date\", col(\"trending_date\").cast(\"timestamp\"))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17854318-9897-484f-a4c4-e18684c0b460",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Combiner for Video Data and Category Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bc1c4a2-f7aa-4ef1-86a5-5a8cdd92c25f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine the Video Data and Category Data\n",
    "def combineData(df1, df2):\n",
    "    # Cast 'categoryId' and 'id' columns to string\n",
    "    df1 = df1.withColumn(\"categoryId\", col(\"categoryId\").cast(\"string\"))\n",
    "    df2 = df2.withColumn(\"id\", col(\"id\").cast(\"string\"))\n",
    "\n",
    "    # Perform the join\n",
    "    df = df1.join(df2, df1.categoryId == df2.id, 'left')\n",
    "\n",
    "    # Drop the redundant columns\n",
    "    df = df.drop(\"categoryId\", \"id\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f9b9d8f-dd69-4ee7-8c12-daaace5c9421",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def process_country_data(country):\n",
    "    video_path = f\"file:/databricks/driver/{country}_youtube_trending_data.csv\"\n",
    "    category_path = f\"file:/databricks/driver/{country}_category_id.json\"\n",
    "    videos = videoDataLoader(video_path,token=False)\n",
    "    data = pd.read_json(category_path)\n",
    "    cid = categoryLoader(data)\n",
    "    \n",
    "    # Combine data and add region column\n",
    "    data = combineData(videos, cid).withColumn(\"region\", lit(country))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d45bdb4-06a6-4d62-9e4c-fefae954a79a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Process & Analysis Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6144d35e-0766-47af-a25d-bbbd4ed4b03f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_data = None\n",
    "for country in ['BR','CA','DE','FR','GB','IN','JP','KR','MX','RU','US']:\n",
    "    country_data = process_country_data(country)\n",
    "    all_data = country_data if all_data is None else all_data.union(country_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3201168d-b2fb-4fe8-8b31-5d7a9e30bc71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "youtuber = spark.read.csv(\"file:/databricks/driver/youtubers_df.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a30eec1-a6c0-43ef-811d-df6a392cf2c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Normalize the 'channelTitle' and 'Username' columns for joining\n",
    "all_data = all_data.withColumn('normalized_channelTitle', lower(regexp_replace('channelTitle', ' ', '')))\n",
    "youtuber = youtuber.withColumn('normalized_Username', lower(regexp_replace('Username', ' ', '')))\n",
    "\n",
    "# Perform the join while handling columns with the same name\n",
    "joined_df = all_data.join(\n",
    "    youtuber,\n",
    "    all_data.normalized_channelTitle == youtuber.normalized_Username\n",
    ").select(\n",
    "    all_data[\"*\"],  # Select all columns from all_data\n",
    "    youtuber[\"Suscribers\"].alias(\"channelSuscribers\"),  # Aliasing Suscribers\n",
    "    youtuber[\"Visits\"].alias(\"channelAvgVisits\"),       # Aliasing Visits\n",
    "    youtuber[\"Likes\"].alias(\"channelAvgLikes\"),         # Aliasing Likes\n",
    "    youtuber[\"Comments\"].alias(\"channelAvgComments\")    # Aliasing Comments\n",
    ")\n",
    "\n",
    "# Select and rename the required columns\n",
    "final_df = joined_df.select(\n",
    "    'video_id', \n",
    "    'title', \n",
    "    'publishedAt', \n",
    "    'channelTitle', \n",
    "    'channelSuscribers', \n",
    "    'category', \n",
    "    'trending_date', \n",
    "    'view_count', \n",
    "    'likes',   # 'likes' from all_data\n",
    "    'dislikes', \n",
    "    'comment_count', \n",
    "    'region',\n",
    "    'channelAvgVisits',\n",
    "    'channelAvgLikes',\n",
    "    'channelAvgComments'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6458572e-b620-4b32-9ecb-c1e0dbda39ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculating durations\n",
    "windowSpec = Window.partitionBy(\"video_id\")\n",
    "final_df = final_df.withColumn(\"min_trending_date\", min(\"trending_date\").over(windowSpec))\n",
    "final_df = final_df.withColumn(\"max_trending_date\", max(\"trending_date\").over(windowSpec))\n",
    "final_df = final_df.withColumn(\"becomeTrendingDuration\", unix_timestamp(\"min_trending_date\") - unix_timestamp(\"publishedAt\"))\n",
    "final_df = final_df.withColumn(\"endTrendingDuration\", unix_timestamp(\"max_trending_date\") - unix_timestamp(\"min_trending_date\"))\n",
    "final_df = final_df.withColumn(\"becomeTrendingDuration\", col(\"becomeTrendingDuration\") / 3600)\n",
    "final_df = final_df.withColumn(\"endTrendingDuration\", col(\"endTrendingDuration\") / 3600)\n",
    "\n",
    "# Step 3: Aggregate data\n",
    "aggregated_data = final_df.groupBy(\"video_id\", \"region\").agg(\n",
    "    first(\"title\").alias(\"title\"),\n",
    "    first(\"publishedAt\").alias(\"publishedAt\"),\n",
    "    first(\"channelTitle\").alias(\"channelTitle\"),\n",
    "    first(\"channelSuscribers\").alias(\"channelSuscribers\"),\n",
    "    first(\"category\").alias(\"category\"),\n",
    "    max(\"view_count\").alias(\"view_count\"),\n",
    "    max(\"likes\").alias(\"likes\"),\n",
    "    max(\"dislikes\").alias(\"dislikes\"),\n",
    "    max(\"comment_count\").alias(\"comment_count\"),\n",
    "    first(\"becomeTrendingDuration\").alias(\"becomeTrendingDuration\"),\n",
    "    max(\"endTrendingDuration\").alias(\"endTrendingDuration\"),\n",
    "    first(\"channelAvgVisits\").alias(\"channelAvgVisits\"),\n",
    "    first(\"channelAvgLikes\").alias(\"channelAvgLikes\"),\n",
    "    first(\"channelAvgComments\").alias(\"channelAvgComments\")\n",
    "    )\n",
    "\n",
    "# Step 4: Final DataFrame with required columns\n",
    "final_result = aggregated_data.select(\n",
    "    \"video_id\", \n",
    "    \"title\", \n",
    "    \"becomeTrendingDuration\", \n",
    "    \"endTrendingDuration\", \n",
    "    \"channelTitle\", \n",
    "    \"channelSuscribers\", \n",
    "    \"category\", \n",
    "    \"region\", \n",
    "    \"view_count\", \n",
    "    \"likes\", \n",
    "    \"dislikes\", \n",
    "    \"comment_count\",\n",
    "    \"channelAvgVisits\",\n",
    "    \"channelAvgLikes\",\n",
    "    \"channelAvgComments\")\n",
    "final_result = final_result.dropna()\n",
    "final_result = final_result.withColumn(\"view_count\", col(\"view_count\").cast(\"long\"))\n",
    "final_result = final_result.withColumn(\"likes\", col(\"likes\").cast(\"long\"))\n",
    "final_result = final_result.withColumn(\"dislikes\", col(\"dislikes\").cast(\"long\"))\n",
    "final_result = final_result.withColumn(\"comment_count\", col(\"comment_count\").cast(\"long\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55137d49-71d5-4547-882d-f25d26a5c827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the result into cloud for future usage\n",
    "sampled_data = final_result.sample(withReplacement=False, fraction=20000/39000)\n",
    "train_data, test_data = sampled_data.randomSplit([0.8, 0.2])\n",
    "train_data.write.parquet(\"dbfs:/FileStore/tables/training.parquet\")\n",
    "test_data.write.parquet(\"dbfs:/FileStore/tables/testing.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93ba178-09cc-465e-a71e-0772ddcce723",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41d92023-e766-414b-8801-4cf2eec66287",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f64fb35f-0559-47af-83d2-9bd520167263",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Content-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12afa3c8-d3db-41be-bf4d-bb10030c5203",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ContentBasedPrediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6acf1db-a2bf-44f5-84d2-2a6644153cc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_data = spark.read.parquet(\"dbfs:/FileStore/tables/training.parquet\")\n",
    "test_data = spark.read.parquet(\"dbfs:/FileStore/tables/testing.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744ec2c0-0472-4d47-b231-26363fc7d643",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "# StringIndexers and OneHotEncoders for categorical features\n",
    "categoryIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "regionIndexer = StringIndexer(inputCol=\"region\", outputCol=\"regionIndex\")\n",
    "categoryEncoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "regionEncoder = OneHotEncoder(inputCol=\"regionIndex\", outputCol=\"regionVec\")\n",
    "\n",
    "# Assemble all features into one vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"becomeTrendingDuration\", \"view_count\", \"likes\", \"dislikes\", \"comment_count\", \n",
    "               \"channelAvgVisits\", \"channelAvgLikes\", \"channelAvgComments\", \"categoryVec\", \"regionVec\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Pipeline for transformations\n",
    "pipeline = Pipeline(stages=[categoryIndexer, regionIndexer, categoryEncoder, regionEncoder, assembler, scaler])\n",
    "pipelineModel = pipeline.fit(train_data)\n",
    "\n",
    "# Transform data\n",
    "train_data = pipelineModel.transform(train_data)\n",
    "test_data = pipelineModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20797de9-7169-4cd3-a7ac-9c4475256bbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "from pyspark.sql.types import FloatType\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate Euclidean distance\n",
    "def euclidean_distance(vec1, vec2):\n",
    "    return float(np.sqrt(np.sum((np.array(vec1) - np.array(vec2))**2)))\n",
    "\n",
    "# Register the UDF\n",
    "distance_udf = udf(euclidean_distance, FloatType())\n",
    "\n",
    "train_data = train_data.withColumnRenamed(\"scaledFeatures\", \"scaledFeatures_train\")\n",
    "test_data = test_data.withColumnRenamed(\"scaledFeatures\", \"scaledFeatures_test\")\n",
    "test_data = test_data.withColumnRenamed(\"video_id\", \"test_video_id\")\n",
    "\n",
    "# Add unique IDs\n",
    "train_data = train_data.withColumn(\"train_id\", monotonically_increasing_id())\n",
    "test_data = test_data.withColumn(\"test_id\", monotonically_increasing_id())\n",
    "\n",
    "# Perform cross join\n",
    "cross_joined = test_data.crossJoin(train_data)\n",
    "\n",
    "# Calculate distances\n",
    "cross_joined = cross_joined.withColumn(\"distance\", distance_udf(col(\"scaledFeatures_test\"), col(\"scaledFeatures_train\")))\n",
    "\n",
    "# Create the distance dataframe\n",
    "distance_df = cross_joined.select(\"test_video_id\",\"test_id\", \"train_id\", \"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47e87140-7cd8-474e-a977-70bb30c30b94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Window function to rank by distance\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "windowSpec = Window.partitionBy(\"test_id\").orderBy(\"distance\")\n",
    "\n",
    "k = 5\n",
    "\n",
    "# Rank the distances and keep only top K neighbors\n",
    "nearest_neighbors_df = distance_df.withColumn(\"rank\", row_number().over(windowSpec)).filter(col(\"rank\") <= k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a23ea599-2bff-4947-8af7-fd56191f60df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Join with train data to get endTrendingDuration\n",
    "prediction_df = nearest_neighbors_df.join(train_data, nearest_neighbors_df.train_id == train_data.train_id, \"inner\")\n",
    "\n",
    "# Group by test data ID and calculate average endTrendingDuration\n",
    "final_predictions = prediction_df.groupBy(\"test_id\").agg(avg(\"endTrendingDuration\").alias(\"predicted_endTrendingDuration\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3527f33-fc6c-4608-b4f6-2362d5528871",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 45.35510204081633\nMean Squared Error: 4174.001632653061\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "merged_df = final_predictions.join(test_data, \"test_id\", \"inner\")\n",
    "\n",
    "# Select only the desired columns: endTrendingDuration and predicted_endTrendingDuration\n",
    "result_df = merged_df.select(\"endTrendingDuration\", \"predicted_endTrendingDuration\")\n",
    "\n",
    "# Evaluators for MAE and MSE\n",
    "mae_evaluator = RegressionEvaluator(labelCol=\"endTrendingDuration\", predictionCol=\"predicted_endTrendingDuration\", metricName=\"mae\")\n",
    "mse_evaluator = RegressionEvaluator(labelCol=\"endTrendingDuration\", predictionCol=\"predicted_endTrendingDuration\", metricName=\"mse\")\n",
    "\n",
    "mae = mae_evaluator.evaluate(result_df)\n",
    "mse = mse_evaluator.evaluate(result_df)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcc850fa-4812-4f40-bb3c-9c0e5b5f0d40",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0505bdc5-3d8f-4d6b-9200-5f4805e19742",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9adae84-0163-4bc3-98e6-5c4a0b961363",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the data with encodings\n",
    "categoryIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "categoryEncoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "\n",
    "regionIndexer = StringIndexer(inputCol=\"region\", outputCol=\"regionIndex\")\n",
    "regionEncoder = OneHotEncoder(inputCol=\"regionIndex\", outputCol=\"regionVec\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"becomeTrendingDuration\", \n",
    "               \"view_count\", \n",
    "               \"likes\", \"dislikes\", \n",
    "               \"comment_count\", \n",
    "               \"categoryVec\", \n",
    "               \"regionVec\",\n",
    "               \"channelAvgVisits\",\n",
    "               \"channelAvgLikes\",\n",
    "               \"channelAvgComments\"], \n",
    "    outputCol=\"features\")\n",
    "\n",
    "# Random Forest model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"endTrendingDuration\")\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[categoryIndexer, categoryEncoder, regionIndexer, regionEncoder, assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33dd82c7-2339-4601-94d6-15224caa45b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_df = spark.read.parquet(\"dbfs:/FileStore/tables/training.parquet\")\n",
    "test_df = spark.read.parquet(\"dbfs:/FileStore/tables/testing.parquet\")\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12b70c8d-c46b-48fb-848d-575db5b7eed8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 39.51241971353258\nMean Squared Error (MSE): 3057.1680142519713\nCoefficient of Determination (R^2): 0.6734392483172081\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using your trained model\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluators for different metrics\n",
    "evaluatorMAE = RegressionEvaluator(labelCol=\"endTrendingDuration\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "evaluatorMSE = RegressionEvaluator(labelCol=\"endTrendingDuration\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "evaluatorR2 = RegressionEvaluator(labelCol=\"endTrendingDuration\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Calculate the metrics\n",
    "mae = evaluatorMAE.evaluate(predictions)\n",
    "mse = evaluatorMSE.evaluate(predictions)\n",
    "r2 = evaluatorR2.evaluate(predictions)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Coefficient of Determination (R^2): {r2}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "TrendingDurationPredication",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
